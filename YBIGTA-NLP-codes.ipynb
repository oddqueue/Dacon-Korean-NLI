{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpY-hSPzYDpj"
   },
   "source": [
    "## Import Package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3007,
     "status": "ok",
     "timestamp": 1645801210333,
     "user": {
      "displayName": "Sangkyu Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09682951002772598371"
     },
     "user_tz": -540
    },
    "id": "WTC8el2vzinc",
    "outputId": "ef8dfadb-7f50-4e52-9427-6ae1fabe73e7"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('./MyDrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5704,
     "status": "ok",
     "timestamp": 1645801201220,
     "user": {
      "displayName": "Sangkyu Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09682951002772598371"
     },
     "user_tz": -540
    },
    "id": "nCW_Js37r9CI",
    "outputId": "b019853f-cfca-4de1-bcb3-16b0f4fc7282"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3885,
     "status": "ok",
     "timestamp": 1645801205102,
     "user": {
      "displayName": "Sangkyu Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09682951002772598371"
     },
     "user_tz": -540
    },
    "id": "KoTOxyzyuATc",
    "outputId": "756dc49d-8738-4a4f-fc7b-c01374cca2ee"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install selenium\n",
    "!apt-get update\n",
    "!apt install chromium-chromedriver\n",
    "!pip install pororo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Xhz9OFjEvzm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.cuda.amp as amp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer, AdamW, get_scheduler\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer, EarlyStoppingCallback, TrainerCallback, TrainerControl\n",
    "\n",
    "import pororo\n",
    "from pororo import Pororo\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvKsvMWUDE85"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "\n",
    "train_pre = train['premise'].str.cat(sep='')\n",
    "train_hyp = train['hypothesis'].str.cat(sep='')\n",
    "\n",
    "train_union = set.union(set(train_pre), set(train_hyp))\n",
    "print(''.join(sorted(train_union)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = train['premise'] + train['hypothesis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KLUE Official Dev Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://aistages-prod-server-public.s3.amazonaws.com/app/Competitions/000068/data/klue-nli-v1.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = tarfile.open('klue-nli-v1.1.tar.gz')\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "\n",
    "with open('klue-nli-v1.1/klue-nli-v1.1_dev.json') as f:\n",
    "  df = pd.DataFrame(json.load(f))\n",
    "\n",
    "KLUE = pd.DataFrame(df[['premise', 'hypothesis', 'gold_label']])\n",
    "KLUE.columns = ['premise', 'hypothesis', 'label']\n",
    "\n",
    "KLUE.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KLUE 데이터셋 단어 집합 확인\n",
    "\n",
    "KLUE_pre = KLUE['premise'].str.cat(sep='')\n",
    "KLUE_hyp = KLUE['hypothesis'].str.cat(sep='')\n",
    "KLUE_union = set.union(set(KLUE_pre), set(KLUE_hyp))\n",
    "print(''.join(sorted(KLUE_union)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KorNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown https://drive.google.com/uc?id=1UJKeJneCtKt_bSH_CXqv_gcMxH7ThfMf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = tarfile.open('/content/drive/MyDrive/dacon/KorNLI.tar')\n",
    "tar.extractall('KorNLI')\n",
    "tar.close()\n",
    "\n",
    "with open('KorNLI/xnli.dev.ko.tsv') as f:\n",
    "  nli_devfile = f.read().splitlines()\n",
    "\n",
    "with open('KorNLI/xnli.test.ko.tsv') as g:\n",
    "  nli_testfile = g.read().splitlines()\n",
    "     \n",
    "korNLI_dev_list = [i.split('\\t') for i in nli_devfile]\n",
    "korNLI_test_list = [i.split('\\t') for i in nli_testfile]\n",
    "\n",
    "korNLI_dev = pd.DataFrame(korNLI_dev_list[1:], columns=korNLI_dev_list[0])\n",
    "korNLI_test = pd.DataFrame(korNLI_test_list[1:], columns=korNLI_test_list[0])\n",
    "\n",
    "korNLI = pd.concat([korNLI_dev, korNLI_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI.columns = ['premise', 'hypothesis', 'label']\n",
    "\n",
    "korNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI_pre = korNLI['premise'].str.cat(sep='')\n",
    "korNLI_hyp = korNLI['hypothesis'].str.cat(sep='')\n",
    "\n",
    "korNLI_union = set.union(set(korNLI_pre), set(korNLI_hyp))\n",
    "print(''.join(sorted(korNLI_union)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 따옴표 변경\n",
    "apostrophes = ['‘', '’']\n",
    "for apostrophe in apostrophes:\n",
    "  korNLI['hypothesis'] = korNLI['hypothesis'].str.replace(apostrophe, '\\'')\n",
    "  korNLI['premise'] = korNLI['premise'].str.replace(apostrophe, '\\'')\n",
    "\n",
    "apostrophes = ['“', '”']\n",
    "for apostrophe in apostrophes:\n",
    "  korNLI['hypothesis'] = korNLI['hypothesis'].str.replace(apostrophe, '\\\"')\n",
    "  korNLI['premise'] = korNLI['premise'].str.replace(apostrophe, '\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI[korNLI['premise'].str.contains('=')].iloc[0]['premise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) 잘라버리기 (726, 727, 728)\n",
    "korNLI.loc[korNLI['premise'].str.contains('='), 'premise'] = korNLI[korNLI['premise'].str.contains('=')]['premise'].str[3:].copy()\n",
    "korNLI[korNLI['premise'].str.contains('=')].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI[korNLI['premise'].str.contains('…')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...  는 .로 대체\n",
    "korNLI['premise'] = korNLI['premise'].str.replace(\"…\", \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI[korNLI['premise'].str.contains('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI.loc[korNLI['premise'].str.contains('_'), 'premise'] = korNLI[korNLI['premise'].str.contains('_')]['premise'].str[:4] + korNLI[korNLI['premise'].str.contains('_')]['premise'].str[8:]\n",
    "korNLI[korNLI['premise'].str.contains('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI['premise'] = korNLI['premise'].str.replace(\"McCohe__\", \"McCoy는 The\")\n",
    "korNLI.iloc[1923]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI[korNLI['premise'].str.contains('·')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI['premise'] = korNLI['premise'].str.replace(\"·\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI[korNLI['premise'].str.contains('『')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI[korNLI['premise'].str.contains('《')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI['premise'] = korNLI['premise'].str.replace(\"[《》『』]\", \"'\")\n",
    "korNLI['hypothesis'] = korNLI['hypothesis'].str.replace(\"[《》『』]\", \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI[korNLI['premise'].str.contains('`')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI.iloc[7155]['premise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI['premise'] = korNLI['premise'].str.replace(\"`\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI[korNLI['premise'].str.contains('\\$')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI[korNLI['hypothesis'].str.contains('\\$')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI['premise'] = korNLI['premise'].str.replace(\"00 생활보호를\", \"생활보호를\")\n",
    "korNLI['premise'] = korNLI['premise'].str.replace(\"\\$3을\", \"3달러를\")\n",
    "korNLI['premise'] = korNLI['premise'].str.replace(\"\\$-\", \"돈\")\n",
    "korNLI['hypothesis'] = korNLI['hypothesis'].str.replace(\"\\$500를\", \"500달러를\")\n",
    "korNLI['hypothesis'] = korNLI['hypothesis'].str.replace(\"\\$500이\", \"500달러가\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI.iloc[6214]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변경된 데이터셋 단어 집합 확인\n",
    "korNLI_dropped_pre = korNLI['premise'].str.cat(sep='')\n",
    "korNLI_dropped_hyp = korNLI['hypothesis'].str.cat(sep='')\n",
    "korNLI_dropped_union = set.union(set(korNLI_dropped_pre), set(korNLI_dropped_hyp))\n",
    "print(''.join(sorted(korNLI_dropped_union)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI[korNLI['premise'].str.contains('\\[')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI['premise'] = korNLI['premise'].str.replace(\"\\[\", \"\")\n",
    "korNLI['premise'] = korNLI['premise'].str.replace(\"\\]\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI['premise'] = korNLI['premise'].str.replace(\";\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI[korNLI['premise'].str.contains('/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI.drop(korNLI[korNLI['premise'].str.contains('/')].index, inplace=True)\n",
    "korNLI.drop(korNLI[korNLI['hypothesis'].str.contains('/')].index, inplace=True)\n",
    "korNLI = korNLI.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI[korNLI['premise'].str.contains('-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변경된 데이터셋 단어 집합 확인\n",
    "korNLI_dropped_pre = korNLI['premise'].str.cat(sep='')\n",
    "korNLI_dropped_hyp = korNLI['hypothesis'].str.cat(sep='')\n",
    "korNLI_dropped_union = set.union(set(korNLI_dropped_pre), set(korNLI_dropped_hyp))\n",
    "print(''.join(sorted(korNLI_dropped_union)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI['index'] = korNLI.index\n",
    "korNLI = korNLI[['index', 'premise', 'hypothesis', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korNLI.to_csv('korNLI_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chrome_setting():\n",
    "  chrome_options = webdriver.ChromeOptions()\n",
    "  chrome_options.add_argument('--headless')\n",
    "  chrome_options.add_argument('--no-sandbox')\n",
    "  chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "  driver = webdriver.Chrome('chromedriver', options=chrome_options)\n",
    "  return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=chrome_setting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kor_to_trans(text_data, trans_lang,start_index,final_index):\n",
    "\n",
    "  target_present = EC.presence_of_element_located((By.XPATH, '//*[@id=\"txtTarget\"]'))\n",
    "\n",
    "  for i in tqdm(range(start_index,final_index)): \n",
    "    \n",
    "    if (i!=0)&(i%99==0):\n",
    "      time.sleep(2)\n",
    "      print('{}th : '.format(i), backtrans)\n",
    "      np.save(data_path+'kor_to_eng_train_{}_{}.npy'.format(start_index,final_index),trans_list)\n",
    "    \n",
    "    try:\n",
    "      driver.get('https://papago.naver.com/?sk=ko&tk='+trans_lang+'&st='+text_data[i])\n",
    "      time.sleep(1.5)\n",
    "      element=WebDriverWait(driver, 10).until(target_present)\n",
    "      time.sleep(0.1)\n",
    "      backtrans = element.text \n",
    "\n",
    "      if (backtrans=='')|(backtrans==' '):\n",
    "        element=WebDriverWait(driver, 10).until(target_present)\n",
    "        backtrans = element.text \n",
    "        trans_list.append(backtrans)\n",
    "      else:\n",
    "        trans_list.append(backtrans)\n",
    "    \n",
    "    except:\n",
    "      trans_list.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_list=[]\n",
    "kor_to_trans(train['premise'], 'en',0,10000)\n",
    "np.save(data_path+'kor_to_eng_train_0_10000.npy',trans_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_data0=np.load(data_path+'kor_to_eng_train_0_10000.npy')\n",
    "eng_data1=np.load(data_path+'kor_to_eng_train_10000_20000.npy')\n",
    "eng_data2=np.load(data_path+'kor_to_eng_train_20000_all.npy')\n",
    "\n",
    "eng_data=[*eng_data0, *eng_data1, *eng_data2]\n",
    "eng_data=pd.DataFrame(eng_data,columns=['eng_premise'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_train=pd.concat([train,eng_data],axis=1)\n",
    "back_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset selenium chrome driver\n",
    "driver=chrome_setting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_to_kor(transed_list, transed_lang,start_index,final_index): \n",
    "  \n",
    "  target_present = EC.presence_of_element_located((By.XPATH, '//*[@id=\"txtTarget\"]'))\n",
    "\n",
    "  for i in tqdm(range(start_index,final_index)): \n",
    "    \n",
    "    if (i!=0)&(i%99==0):\n",
    "      time.sleep(1.5)\n",
    "      print('{}th : '.format(i), backtrans)\n",
    "      np.save(data_path+'eng_to_kor_train_{}_{}.npy'.format(start_index,final_index),trans_list)\n",
    "    \n",
    "    try:\n",
    "      driver.get('https://papago.naver.com/?sk=en&tk='+transed_lang+'&st='+transed_list[i])\n",
    "      time.sleep(2)\n",
    "      element=WebDriverWait(driver, 10).until(target_present)\n",
    "      time.sleep(0.2)\n",
    "      backtrans = element.text \n",
    "\n",
    "      if (backtrans=='')|(backtrans==' '):\n",
    "        element=WebDriverWait(driver, 10).until(target_present)\n",
    "        backtrans = element.text \n",
    "        trans_list.append(backtrans)\n",
    "      else:\n",
    "        trans_list.append(backtrans)\n",
    "    \n",
    "    except:\n",
    "      trans_list.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_list=[]\n",
    "trans_to_kor(back_train['eng_premise'], 'ko',0,10000)\n",
    "np.save(data_path+'eng_to_kor_train_0_10000.npy',trans_list)\n",
    "\n",
    "trans_list=[]\n",
    "trans_to_kor(back_train['eng_premise'], 'ko',10000,20000)\n",
    "np.save(data_path+'eng_to_kor_train_10000_20000.npy',trans_list)\n",
    "\n",
    "trans_list=[]\n",
    "trans_to_kor(back_train['eng_premise'], 'ko',20000,len(back_train))\n",
    "np.save(data_path+'eng_to_kor_train_20000_all.npy',trans_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back0=np.load(data_path+'eng_to_kor_train_0_10000.npy')\n",
    "back1=np.load(data_path+'eng_to_kor_train_10000_20000.npy')\n",
    "back2=np.load(data_path+'eng_to_kor_train_20000_all.npy')\n",
    "back_train_fin=[*back0,*back1,*back2]\n",
    "back_train_fin=pd.DataFrame(back_train_fin,columns=['back_premise'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_train_fin=pd.concat([train,back_train_fin],axis=1)\n",
    "back_train_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(back_train_fin)):\n",
    "  back_train_fin.at[i,'back_premise']=back_train_fin['back_premise'][i].replace('U.S.','미국')\n",
    "  back_train_fin.at[i,'back_premise']=re.sub(r'([.?!/\\\\]+)(?![0-9])', r'\\1 ', back_train_fin['back_premise'][i]).replace('... ','...').replace('  ',' ').replace('.. ','..').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the proportion of Hangul in the total sentence length is 0.6 or higher, it is considered an abnormal translation and an attempt to re-translate it.\n",
    "# -> 전체 문장길이에서 한글가 차지하는 비중이 0.6이상이면 이상번역으로 간주하고 재번역 시도\n",
    "\n",
    "# Attempt to re-translate a translated sentence if the translated sentence has a ratio of less than 0.5 to the length of an existing sentence\n",
    "# -> 번역된 문장이 기존 문장의 길이에 대한 비율이 0.5이하이면 재번역 시도\n",
    "retrans_ind=[]\n",
    "\n",
    "for i in tqdm(range(0,len(back_train_fin))):\n",
    "  kor_ratio=len(re.sub('[a-zA-Z]','',back_train_fin['back_premise'][i]))/(len(back_train_fin['back_premise'][i])+1)\n",
    "  if kor_ratio<0.6:\n",
    "    retrans_ind.append(i)\n",
    "  if len(back_train_fin['back_premise'][i])/len(back_train_fin['premise'][i])<=0.5:\n",
    "    retrans_ind.append(i)\n",
    "\n",
    "retrans_ind=list(set(retrans_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_trans = pd.read_csv('back_trans.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변경된 데이터셋 단어 집합 확인\n",
    "back_trans_pre = back_trans['premise'].str.cat(sep='')\n",
    "back_trans_hyp = back_trans['hypothesis'].str.cat(sep='')\n",
    "back_trans_all = set.union(set(back_trans_pre), set(back_trans_hyp))\n",
    "print(''.join(sorted(back_trans_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_trans[back_trans['premise'].str.contains('故')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_trans[back_trans['hypothesis'].str.contains('故')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_trans['premise'] = back_trans['premise'].str.replace(\"故\", '고')\n",
    "back_trans['premise'] = back_trans['premise'].str.replace(\"京義線\", ' ')\n",
    "back_trans['premise'] = back_trans['premise'].str.replace(\"九州\", ' ')\n",
    "back_trans['premise'] = back_trans['premise'].str.replace(\"㎡\", '제곱미터')\n",
    "back_trans['premise'] = back_trans['premise'].str.replace(\"㎞\", '킬로미터')\n",
    "back_trans['hypothesis'] = back_trans['hypothesis'].str.replace('ㄷ', ' ')\n",
    "back_trans['hypothesis'] = back_trans['hypothesis'].str.replace('옞ㅇ', '예정')\n",
    "back_trans['hypothesis'] = back_trans['hypothesis'].str.replace('%', '퍼센트')\n",
    "back_trans['premise'] = back_trans['premise'].str.replace(\"%\", '퍼센트')\n",
    "back_trans['hypothesis'] = back_trans['hypothesis'].str.replace('[「」]', ' ')\n",
    "back_trans['premise'] = back_trans['premise'].str.replace(\"[「」]\", ' ')\n",
    "back_trans['hypothesis'] = back_trans['hypothesis'].str.replace('[ᅵ…]', ' ')\n",
    "back_trans['premise'] = back_trans['premise'].str.replace(\"[ᅵ…]\", ' ')\n",
    "back_trans['hypothesis'] = back_trans['hypothesis'].str.replace('[<>]', ' ')\n",
    "back_trans['premise'] = back_trans['premise'].str.replace(\"[<>]\", ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변경된 데이터셋 단어 집합 확인\n",
    "back_trans_pre = back_trans['premise'].str.cat(sep='')\n",
    "back_trans_hyp = back_trans['hypothesis'].str.cat(sep='')\n",
    "back_trans_all = set.union(set(back_trans_pre), set(back_trans_hyp))\n",
    "print(''.join(sorted(back_trans_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_trans[back_trans['premise'].str.contains('\\*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_trans['premise'] = back_trans['premise'].str.replace(\"\\*\", '원이 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_trans.iloc[1444]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변경된 데이터셋 단어 집합 확인\n",
    "back_trans_pre = back_trans['premise'].str.cat(sep='')\n",
    "back_trans_hyp = back_trans['hypothesis'].str.cat(sep='')\n",
    "back_trans_all = set.union(set(back_trans_pre), set(back_trans_hyp))\n",
    "print(''.join(sorted(back_trans_all)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.concat([train, KLUE, korNLI, back_trans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.reset_index(drop=True)\n",
    "output['index'] = output.index\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 훈련 데이터셋 단어 집합 확인\n",
    "output_pre = output['premise'].str.cat(sep='')\n",
    "output_hyp = output['hypothesis'].str.cat(sep='')\n",
    "output_union = set.union(set(output_pre), set(output_hyp))\n",
    "print(''.join(sorted(output_union)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.reset_index(drop=True)\n",
    "output.drop('index', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('train+KLUE+korNLI+back_trans(processed).csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1645801210334,
     "user": {
      "displayName": "Sangkyu Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09682951002772598371"
     },
     "user_tz": -540
    },
    "id": "ldTRvy0bXqCz",
    "outputId": "3724d648-d6a8-4e6c-eb1e-37ef6922887a"
   },
   "outputs": [],
   "source": [
    "PATH = '/content/MyDrive/MyDrive/Colab Notebooks/dacon'\n",
    "\n",
    "train = pd.read_csv(os.path.join(PATH, 'data/train+KLUE+korNLI+back_trans(processed).csv'), encoding='utf-8')\n",
    "test = pd.read_csv(os.path.join(PATH, 'data/test_data.csv'), encoding='utf-8')\n",
    "\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = train['premise'].str.cat(sep='')\n",
    "hyp = train['hypothesis'].str.cat(sep='')\n",
    "\n",
    "union = set.union(set(pre), set(hyp))\n",
    "print(''.join(sorted(union)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1645801211177,
     "user": {
      "displayName": "Sangkyu Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09682951002772598371"
     },
     "user_tz": -540
    },
    "id": "Sd9MmfWcUtkH",
    "outputId": "6bdb9286-b19d-4dc5-bcf4-cddcd3066428"
   },
   "outputs": [],
   "source": [
    "print(train[train['premise'].str.contains('ㄷ')])\n",
    "print(train[train['premise'].str.contains('ㅇ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1645801211177,
     "user": {
      "displayName": "Sangkyu Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09682951002772598371"
     },
     "user_tz": -540
    },
    "id": "Sa8L_G5xUSg_",
    "outputId": "338ef3d2-4d8d-4450-e507-0932c8120cc7"
   },
   "outputs": [],
   "source": [
    "print(train[train['hypothesis'].str.contains('ㄷ')])\n",
    "print(train[train['hypothesis'].str.contains('ㅇ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1645801211177,
     "user": {
      "displayName": "Sangkyu Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09682951002772598371"
     },
     "user_tz": -540
    },
    "id": "QYh6Plz3vVZA",
    "outputId": "576f52ce-55d6-405f-f613-32cb12203071"
   },
   "outputs": [],
   "source": [
    "train['hypothesis'].iloc[20415] = train['hypothesis'].iloc[20415].replace(\"ㄷ.\", \"\")\n",
    "train['hypothesis'].iloc[12955] = train['hypothesis'].iloc[12955].replace(\"옞ㅇ\", \"예정\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTDAsjbeEjIJ"
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jh16K_pjFS11"
   },
   "source": [
    "### Fixing Seed/GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1645801211178,
     "user": {
      "displayName": "Sangkyu Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09682951002772598371"
     },
     "user_tz": -540
    },
    "id": "cwLrRTFkzWYu",
    "outputId": "4a468a2a-51d5-487b-b68f-1e63c68032e9"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed:int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  \n",
    "    torch.backends.cudnn.deterministic = True \n",
    "    torch.backends.cudnn.benchmark = True \n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_G-UXaLSF5g5"
   },
   "source": [
    "### Tokenizing/Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59sIma4zTLTh"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'klue/roberta-large'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGD8NaEKsjd1"
   },
   "outputs": [],
   "source": [
    "class BERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, pair_dataset, label):\n",
    "        self.pair_dataset = pair_dataset\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
    "        item['label'] = torch.tensor(self.label[idx])\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOZBS0IbL4kg"
   },
   "outputs": [],
   "source": [
    "def label_to_num(label):\n",
    "    label_dict = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2, \"answer\": 3}\n",
    "    num_label = []\n",
    "\n",
    "    for v in label:\n",
    "        num_label.append(label_dict[v])\n",
    "    \n",
    "    return num_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6OAcOq6D236"
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "folds = []\n",
    "for train_idx, eval_idx in skf.split(train, train['label']):\n",
    "    train_data = train.iloc[train_idx]\n",
    "    eval_data = train.iloc[eval_idx]\n",
    "\n",
    "    folds.append([train_data, eval_data])\n",
    "\n",
    "tokens = []\n",
    "for i in range(len(folds)):\n",
    "    tokenized_train = tokenizer(\n",
    "    list(folds[i][0]['premise']),\n",
    "    list(folds[i][0]['hypothesis']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=256,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    "    )\n",
    "\n",
    "    tokenized_eval = tokenizer(\n",
    "    list(folds[i][1]['premise']),\n",
    "    list(folds[i][1]['hypothesis']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=256,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    "    )\n",
    "\n",
    "    tokens.append((tokenized_train, tokenized_eval))\n",
    "\n",
    "label_folds = []\n",
    "for i in range(len(folds)):\n",
    "    train_label = label_to_num(folds[i][0]['label'].values)\n",
    "    eval_label = label_to_num(folds[i][1]['label'].values)\n",
    "\n",
    "    label_folds.append([train_label, eval_label])\n",
    "\n",
    "dataset_folds = []\n",
    "for i in range(len(tokens)):\n",
    "    train_dataset = BERTDataset(tokens[i][0], label_folds[i][0])\n",
    "    eval_dataset = BERTDataset(tokens[i][1], label_folds[i][1])\n",
    "\n",
    "    dataset_folds.append([train_dataset, eval_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1645801240750,
     "user": {
      "displayName": "Sangkyu Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09682951002772598371"
     },
     "user_tz": -540
    },
    "id": "7Nz2QJrAK7DU",
    "outputId": "82ff1340-9ae5-4d4a-8da3-76509398320b"
   },
   "outputs": [],
   "source": [
    "print(dataset_folds[0][0].__len__())\n",
    "print(dataset_folds[0][0].__getitem__(0))\n",
    "print(tokenizer.decode(dataset_folds[0][0].__getitem__(0)['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMCJxB6txaIm"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7OLCtj_k8yiq"
   },
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            if not return_outputs:\n",
    "                normal_emb = model.roberta.embeddings(input_ids=inputs['input_ids'], token_type_ids=inputs['token_type_ids'])\n",
    "                normal_encoder = model.roberta.encoder(normal_emb, encoder_attention_mask=inputs['attention_mask'])\n",
    "                normal_sequence = normal_encoder[0]\n",
    "                logits = model.classifier(normal_sequence)\n",
    "                ce_loss = torch.nn.CrossEntropyLoss()\n",
    "                loss = ce_loss(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "\n",
    "                if self.args.r3f_lambda != 0:\n",
    "                    noise_sampler = torch.distributions.normal.Normal(loc=0.0, scale=1e-5)\n",
    "                    noise = noise_sampler.sample(sample_shape=normal_emb.shape)\n",
    "                    noise_emb = noise.to(normal_emb.get_device()) + normal_emb.detach().clone()\n",
    "                    noise_encoder = model.roberta.encoder(noise_emb, encoder_attention_mask=inputs['attention_mask'])\n",
    "                    noise_sequence = noise_encoder[0]\n",
    "                    noise_logits = model.classifier(noise_sequence)\n",
    "                    loss += self.args.r3f_lambda * self.symm_kl_loss(noise_logits.view(-1, self.model.config.num_labels), logits.view(-1, self.model.config.num_labels))\n",
    "\n",
    "                return loss\n",
    "\n",
    "            else:\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.get(\"logits\")\n",
    "                ce_loss = torch.nn.CrossEntropyLoss()\n",
    "                loss = ce_loss(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "\n",
    "                return (loss, outputs)\n",
    "            \n",
    "    def symm_kl_loss(self, noised_logits, input_logits):\n",
    "      return (\n",
    "          F.kl_div(\n",
    "              F.log_softmax(noised_logits, dim=-1, dtype=torch.float32),\n",
    "              F.softmax(input_logits, dim=-1, dtype=torch.float32),\n",
    "              reduction=\"sum\",\n",
    "          )\n",
    "          + F.kl_div(\n",
    "              F.log_softmax(input_logits, dim=-1, dtype=torch.float32),\n",
    "              F.softmax(noised_logits, dim=-1, dtype=torch.float32),\n",
    "              reduction=\"sum\",\n",
    "          )\n",
    "      ) / noised_logits.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wOaUdvbfO1w0"
   },
   "outputs": [],
   "source": [
    "class CustomCallback(EarlyStoppingCallback):\n",
    "    def check_metric_value(self, args, state, control, metric_value):\n",
    "        operator = np.greater if args.greater_is_better else np.less\n",
    "        if metric_value <= 0.8:\n",
    "            control.should_training_stop = True\n",
    "\n",
    "        elif state.best_metric is None or (\n",
    "            operator(metric_value, state.best_metric)\n",
    "            and abs(metric_value - state.best_metric) > self.early_stopping_threshold\n",
    "        ):\n",
    "            self.early_stopping_patience_counter = 0\n",
    "\n",
    "        else:\n",
    "            self.early_stopping_patience_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRl1gY7SdZWP"
   },
   "outputs": [],
   "source": [
    "class CustomTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if 'r3f_lambda' in kwargs:\n",
    "            self.r3f_lambda = kwargs.pop('r3f_lambda', None)\n",
    "\n",
    "        else:\n",
    "            self.r3f_lambda = 0\n",
    "\n",
    "        super(CustomTrainingArguments, self).__init__(*args, **kwargs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ad-aYSAm7InR"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  acc = accuracy_score(labels, preds) \n",
    "\n",
    "  return {\n",
    "      'accuracy': acc,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2968912,
     "status": "ok",
     "timestamp": 1645816452913,
     "user": {
      "displayName": "Sangkyu Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09682951002772598371"
     },
     "user_tz": -540
    },
    "id": "6zLkAk4A98dl",
    "outputId": "738c5e92-47dc-4777-d85a-73cf8d6e8151"
   },
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "config.num_labels = 3\n",
    "CHECK_PATH = os.path.join(PATH, 'checkpoints')\n",
    "\n",
    "num_epochs = 5\n",
    "r3f_lambda_search = [0, 0.5, 1, 2]\n",
    "optimizer_lr = 1e-5\n",
    "batch_size = 30 # [28, 30, 32]\n",
    "\n",
    "for (train_dataset, eval_dataset), i in zip(dataset_folds, range(1, len(dataset_folds) + 1)):\n",
    "    FOLD_CHECK_PATH = os.path.join(CHECK_PATH, 'fold_'+ str(i))\n",
    "    print('Fold %d training started' %(i))\n",
    "\n",
    "    for j in range(len(r3f_lambda_search)):\n",
    "        PARAMETER_CHECK_PATH = os.path.join(FOLD_CHECK_PATH, 'lambda_' + str(r3f_lambda_search[j]))\n",
    "\n",
    "        training_ars = CustomTrainingArguments(\n",
    "            output_dir=PARAMETER_CHECK_PATH,\n",
    "            num_train_epochs=num_epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            save_total_limit=3,\n",
    "            save_strategy='epoch',\n",
    "            evaluation_strategy='epoch',\n",
    "            load_best_model_at_end = True,\n",
    "            half_precision_backend='amp',\n",
    "            metric_for_best_model= 'accuracy',\n",
    "            r3f_lambda = r3f_lambda_search[j]\n",
    "        )\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "        optimizer = AdamW(model.parameters(), lr=optimizer_lr)\n",
    "        num_training_steps = num_epochs * len(train_dataset) / batch_size\n",
    "        lr_scheduler = get_scheduler(\"polynomial\", optimizer=optimizer, num_warmup_steps=(num_training_steps/batch_size) , num_training_steps=num_training_steps)\n",
    "\n",
    "        trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_ars,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[CustomCallback(1, 0)],\n",
    "            optimizers=(optimizer, lr_scheduler)\n",
    "        )\n",
    "        print('lambda = %.4f training' %(r3f_lambda_search[j]))\n",
    "        trainer.train()\n",
    "        \n",
    "        best_result = trainer.evaluate(eval_dataset)\n",
    "        best_acc = 'best_model(%.4f)' % (best_result['eval_accuracy'])\n",
    "        BEST_PATH = os.path.join(PARAMETER_CHECK_PATH, best_acc)\n",
    "        model.save_pretrained(BEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "434WGH6txdwe"
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9FttWz0g-9Z"
   },
   "source": [
    "### Best Model Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 379,
     "status": "ok",
     "timestamp": 1645818036165,
     "user": {
      "displayName": "Sangkyu Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09682951002772598371"
     },
     "user_tz": -540
    },
    "id": "DQbLwKaNemeK"
   },
   "outputs": [],
   "source": [
    "FOLD_1_BEST =\n",
    "FOLD_2_BEST =\n",
    "FOLD_3_BEST = \n",
    "FOLD_4_BEST = \n",
    "FOLD_5_BEST = \n",
    "\n",
    "BEST_MODELS = [FOLD_1_BEST, FOLD_2_BEST, FOLD_3_BEST, FOLD_4_BEST, FOLD_5_BEST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1645818038824,
     "user": {
      "displayName": "Sangkyu Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09682951002772598371"
     },
     "user_tz": -540
    },
    "id": "CHgoD__nyOiC"
   },
   "outputs": [],
   "source": [
    "def num_to_label(label):\n",
    "    label_dict = {0: \"entailment\", 1: \"contradiction\", 2: \"neutral\"}\n",
    "    str_label = []\n",
    "\n",
    "    for i, v in enumerate(label):\n",
    "        str_label.append([i,label_dict[v]])\n",
    "    \n",
    "    return str_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6786,
     "status": "ok",
     "timestamp": 1645818046023,
     "user": {
      "displayName": "Sangkyu Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09682951002772598371"
     },
     "user_tz": -540
    },
    "id": "S27_TQuNjQTg",
    "outputId": "158082d1-72d2-44c2-f9dd-ea6ac36a9a9e"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenized_test = tokenizer(\n",
    "    list(test['premise']),\n",
    "    list(test['hypothesis']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=256,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "test_label = label_to_num(test['label'].values)\n",
    "test_dataset = BERTDataset(tokenized_test, test_label)\n",
    "dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhBk6UZ5Obgw"
   },
   "source": [
    "### Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127250,
     "status": "ok",
     "timestamp": 1645818244715,
     "user": {
      "displayName": "Sangkyu Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09682951002772598371"
     },
     "user_tz": -540
    },
    "id": "okQ3vBMrp1S-",
    "outputId": "33698eac-6c09-4959-e172-84f458bd223a"
   },
   "outputs": [],
   "source": [
    "ensemble_logits = 0\n",
    "for best_model in BEST_MODELS:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(best_model)\n",
    "    model.resize_token_embeddings(tokenizer.vocab_size)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    logits = []\n",
    "    for i, data in enumerate(tqdm(dataloader)):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=data['input_ids'].to(device),\n",
    "                attention_mask=data['attention_mask'].to(device),\n",
    "                token_type_ids=data['token_type_ids'].to(device)\n",
    "            )\n",
    "        logit = F.softmax(outputs[0], dim=-1)\n",
    "        logits.extend(logit.tolist())\n",
    "  \n",
    "    ensemble_logits += np.array(logits)\n",
    "\n",
    "ensemble_logits = ensemble_logits / len(dataset_folds) \n",
    "ensemble_pred = np.argmax(ensemble_logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1645818270751,
     "user": {
      "displayName": "Sangkyu Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09682951002772598371"
     },
     "user_tz": -540
    },
    "id": "1qbJeQityu7i",
    "outputId": "cc99f97b-50d6-402a-f793-e755892d4929"
   },
   "outputs": [],
   "source": [
    "answer = num_to_label(ensemble_pred)\n",
    "print(answer)\n",
    "\n",
    "df = pd.DataFrame(answer, columns=['index', 'label'])\n",
    "\n",
    "df.to_csv('/content/MyDrive/MyDrive/Colab Notebooks/dacon/submission_ensemble_final.csv', index=False)\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "jpY-hSPzYDpj"
   ],
   "machine_shape": "hm",
   "name": "base_line.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
